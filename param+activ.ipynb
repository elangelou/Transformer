{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import transformer_modules.LayerNorm as LayerNorm\n",
    "import DemoTransformer\n",
    "\n",
    "model_cfg = LayerNorm.Config(\n",
    "    debug=False, \n",
    "    d_model=256, \n",
    "    n_heads=4, \n",
    "    d_head=64, \n",
    "    d_mlp=1024, \n",
    "    n_layers=2, \n",
    "    n_ctx=256, \n",
    "    d_vocab=LayerNorm.reference_gpt2.cfg.d_vocab\n",
    ")\n",
    "model = DemoTransformer(model_cfg)\n",
    "\n",
    "\n",
    "batch = 1\n",
    "position = 35\n",
    "d_model = 768\n",
    "n_heads = 12\n",
    "n_layers = 12\n",
    "d_mlp = 3072 \n",
    "d_head = 64\n",
    "\n",
    "for activation_name, activation in LayerNorm.cache.items():\n",
    "    # Only print for first layer\n",
    "    if \".0.\" in activation_name or \"blocks\" not in activation_name:\n",
    "        print(f\"{activation_name:30} {tuple(activation.shape)}\")\n",
    "\n",
    "for name, param in LayerNorm.reference_gpt2.named_parameters():\n",
    "    # Only print for first layer\n",
    "    if \".0.\" in name or \"blocks\" not in name:\n",
    "        print(f\"{name:18} {tuple(param.shape)}\")\n",
    "\n",
    "print(LayerNorm.reference_gpt2.cfg)\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    d_model: int = 768\n",
    "    debug: bool = True\n",
    "    layer_norm_eps: float = 1e-5\n",
    "    d_vocab: int = 50257\n",
    "    init_range: float = 0.02\n",
    "    n_ctx: int = 1024\n",
    "    d_head: int = 64\n",
    "    d_mlp: int = 3072\n",
    "    n_heads: int = 12\n",
    "    n_layers: int = 12\n",
    "\n",
    "cfg = Config()\n",
    "print(cfg)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
