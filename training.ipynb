{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch as nn\n",
    "import Full_Transformer\n",
    "import wandb\n",
    "import numpy as np\n",
    "import datasets\n",
    "import math\n",
    "from torch import Tensor\n",
    "from typing import Dict\n",
    "import tqdm.auto as tqdm\n",
    "from Full_Transformer import Config, reference_gpt2\n",
    "from Full_Transformer import DemoTransformer\n",
    "from transformer_lens.utils import gelu_new, tokenize_and_concatenate\n",
    "from dataclasses import dataclass\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from LayerNorm import cfg\n",
    "\n",
    "##Config \n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "model_cfg = Config(debug=False, d_model=256, n_heads=4, d_head=64, d_mlp=1024, n_layers=2, n_ctx=256, d_vocab=reference_gpt2.cfg.d_vocab)\n",
    "\n",
    "## Create Data\n",
    "\n",
    "dataset = datasets.load_dataset(\"NeelNanda/pile-10k\", split=\"train\")\n",
    "print(dataset)\n",
    "print(dataset[0]['text'][:100])\n",
    "tokens_dataset = tokenize_and_concatenate(dataset, reference_gpt2.tokenizer, streaming=False, max_length=model_cfg.n_ctx, column_name=\"text\", num_proc=4 )\n",
    "data_loader = torch.utils.data.DataLoader(tokens_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "## Create Model\n",
    "\n",
    "model = DemoTransformer(model_cfg)\n",
    "model.cuda()\n",
    "\n",
    "## Create Optimizer \n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "## Run training loop\n",
    "\n",
    "loses = []\n",
    "print(\"number of batches:\", len(data_loader))\n",
    "for epoch in range(num_epochs):\n",
    "    for c, batch in tqdm.tqdm(enumerate(data_loader)):\n",
    "        tokens = batch['tokens'].cuda()\n",
    "        logits = model(tokens)\n",
    "        loss = lm_cross_entropy_loss(logits, tokens)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        losses.append(loss.item())\n",
    "        if c % log_every == 0:\n",
    "            print(f\"Step: {c}, Loss: {loss.item():.4f}\")\n",
    "        if c > max_steps:\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
